{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#######################################################################\n",
    "# Copyright (C)                                                       #\n",
    "# 2016 Shangtong Zhang(zhangshangtong.cpp@gmail.com)                  #\n",
    "# 2016 Jan Hakenberg(jan.hakenberg@gmail.com)                         #\n",
    "# 2016 Tian Jun(tianjun.cpp@gmail.com)                                #\n",
    "# 2016 Kenta Shimada(hyperkentakun@gmail.com)                         #\n",
    "# update --->                                                         #\n",
    "# 2018 Ran Xiao(xiaoranone@gmail.com)                                 #\n",
    "# python3.5                                                           #\n",
    "#######################################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "强化学习第二章的例子的代码，10臂赌博问题，首先建立一个k臂赌博者的类。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Bandit:\n",
    "    '''参数：\n",
    "        kArm: int, 赌博臂的个数\n",
    "        epsilon: double, e-贪心算法的概率值\n",
    "        initial: 每个行为的行为的初始化估计\n",
    "        stepSize: double,更加估计值的常数步数\n",
    "        sampleAverages: if ture, 使用简单的均值方法替代stepSize权重更新\n",
    "        UCB: 不是None时，使用UCB算法,(初始值优化算法)\n",
    "        gradient: if ture, 使用算法的选择的基础标志(过去的均值作为基准，评价现在的值)\n",
    "        gradientBaseline: if true, 使用过去的奖励的平均值\n",
    "    '''\n",
    "    def __init__(self, kArm=10, epsilon=0., initial=0., stepSize=0.1, \n",
    "                 sampleAverages=False,UCB=None, gradient=False, \n",
    "                 gradientBaseline=False, trueReward=0.):\n",
    "        self.k = kArm\n",
    "        self.epsilon = epsilon\n",
    "        self.stepSize = stepSize\n",
    "        self.sampleAverages = sampleAverages\n",
    "        self.indices = np.arange(self.k) # 有kArm个选择\n",
    "        self.time = 0 # 总的选择次数 ？？\n",
    "        self.UCB = UCB\n",
    "        self.gradient = gradient\n",
    "        self.gradientBaseline = gradientBaseline\n",
    "        self.averageReward = 0\n",
    "        self.trueReward = trueReward\n",
    "        \n",
    "        # 记录每个行为的真实奖励\n",
    "        self.qTrue = []\n",
    "        \n",
    "        # 记录每个行为的估计值\n",
    "        self.qEst = np.zeros(self.k)\n",
    "        \n",
    "        # 记录每个行为被选择的次数\n",
    "        self.actionCount = []\n",
    "        \n",
    "        # 使用N(0,1)高斯分布+trueReward，初始化真是的奖励\n",
    "        # 使用初始值initial初始化估计值\n",
    "        for i in range(0,self.k):\n",
    "            self.qTrue.append(np.random.randn()+trueReward)\n",
    "            self.qEst[i] = initial\n",
    "            self.actionCount.append(0)\n",
    "        \n",
    "        # 得到正在的最好的选择对饮的k臂\n",
    "        self.bestAction = np.argmax(self.qTrue)\n",
    "    \n",
    "    # 对于这个bandit游戏，选择一个行为，使用explore(评估) or exploit(探索)\n",
    "    def getAction(self):\n",
    "        # explore(评估)\n",
    "        # 使用epsilon-greedy算法，每次以概率epsilon随机选择一个行为，\n",
    "        # 否则使用贪心规则\n",
    "        if self.epsilon > 0: \n",
    "            if np.random.binomial(1,self.epsilon) == 1:# 打乱，随机选择\n",
    "                np.random.shuffle(self.indices)\n",
    "                return self.indices[0]\n",
    "        \n",
    "        # exploit\n",
    "        # 使用初始值优化这个算法\n",
    "        if self.UCB is not None:\n",
    "            UCBEst = self.qEst + self.UCB * np.sqrt(np.log(self.time+1) / np.asarray(self.actionCount)+1)\n",
    "            return np.argmax(UCBEst)\n",
    "        \n",
    "        # 使用基准线评测,增强比较\n",
    "        if self.gradient:\n",
    "            # softmax计算每个行为的偏好程度\n",
    "            expEst = np.exp(self.qEst)\n",
    "            self.actionProb = expEst / np.sum(expEst)\n",
    "            # 根据概率随机选择\n",
    "            return np.random.choice(self.indices,p=self.actionProb)\n",
    "        # 选择最大值的下标\n",
    "        return np.argmax(self.qEst)\n",
    "    \n",
    "    # 采取何种行为\n",
    "    def takeAction(self, action):\n",
    "        # 基于N(real reward, 1)产生一个奖励\n",
    "        reward = np.random.randn() + self.qTrue[action]\n",
    "        # 次数加1\n",
    "        self.time += 1\n",
    "        # 迭代计算平均奖励\n",
    "        self.averageReward = (self.time - 1.0) / self.time * self.averageReward + reward / self.time\n",
    "        self.actionCount[action] += 1\n",
    "        \n",
    "        if self.sampleAverages:\n",
    "            # 使用简单平均值更新估计值\n",
    "            self.qEst[action] += 1.0 / self.actionCount[action] * (reward - self.qEst[action])\n",
    "        elif self.gradient:\n",
    "            oneHot = np.zeros(self.k)\n",
    "            oneHot[action] = 1\n",
    "            if self.gradientBaseline:\n",
    "                baseline = gradientBaseline\n",
    "            else:\n",
    "                baseline = 0\n",
    "            # 基于选择，全部更新值，选中的action进行加，没有选中的进行减去一个值\n",
    "            self.qEst = self.qEst + self.stepSize * (reward - baseline) * (oneHot - self.actionProb)\n",
    "        else:\n",
    "            # 固定步长更新值\n",
    "            self.qEst += self.stepSize * (reward - self.qEst[action])\n",
    "        \n",
    "        return reward\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "figureIndex = 0\n",
    "\n",
    "# 做出对应的图，figure 2.1\n",
    "def figure2_1():\n",
    "    global figureIndex\n",
    "    figureIndex += 1\n",
    "    sns.violinplot(data=np.random.randn(200,10) + np.random.randn(10))\n",
    "    plt.xlabel('Action')\n",
    "    plt.ylabel('Reward distribution')\n",
    "\n",
    "\n",
    "def banditSimulation(nBandits, time, bandits):\n",
    "    bestActionCounts = [np.zeros(time, dtype='float') for _ in range(0, len(bandits))]\n",
    "    averageRewards = [np.zeros(time, dtype='float') for _ in range(0, len(bandits))]\n",
    "    for banditInd, bandit in enumerate(bandits):\n",
    "        for i in range(0, nBandits):\n",
    "            for t in range(0, time):\n",
    "                action = bandit[i].getAction()\n",
    "                reward = bandit[i].takeAction(action)\n",
    "                averageRewards[banditInd][t] += reward\n",
    "                if action == bandit[i].bestAction:\n",
    "                    bestActionCounts[banditInd][t] += 1\n",
    "        bestActionCounts[banditInd] /= nBandits\n",
    "        averageRewards[banditInd] /= nBandits\n",
    "    return bestActionCounts, averageRewards\n",
    "\n",
    "\n",
    "# for figure 2.2\n",
    "def epsilonGreedy(nBandits, time):\n",
    "    epsilons = [0, 0.1, 0.01]\n",
    "    # 赌博的个数\n",
    "    bandits = []\n",
    "    for epsInd, eps in enumerate(epsilons):\n",
    "        bandits.append([Bandit(epsilon=eps, sampleAverages=True) for _ in range(0,nBandits)])\n",
    "    \n",
    "    bestActionCounts, avetageReward = banditSimulation(nBandits, time, bandits)\n",
    "    global figureIndex\n",
    "    plt.figure(figureIndex)\n",
    "    figureIndex += 1\n",
    "    for eps, counts in zip(epsilons, beatActionCounts):\n",
    "        plt.plot(counts, label='epsilon='+str(eps))\n",
    "    plt.xlabel('Steps')\n",
    "    plt.ylabel('% optimal action')\n",
    "    plt.legend()\n",
    "    plt.figure(figureIndex)\n",
    "    figureIndex += 1\n",
    "    for eps, reward in zip(epsilons, avetageReward):\n",
    "        plt.plot(reward, label='epsolon='+str(eps))\n",
    "    plt.xlabel('Steps')\n",
    "    plt.ylabel('average reward')\n",
    "    plt.legend()\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "figure2_1()\n",
    "\n",
    "epsilonGreedy(2000,1000)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
